La ciencia de datos es un campo académico interdisciplinario que utiliza estadística, computación científica, métodos, procesos, algoritmos y sistemas científicos para obtener (recolectar o extraer), tratar, analizar y presentar informes a partir de datos ruidosos, estructurados y no estructurados. 

La ciencia de datos es multifacética y puede describirse como una ciencia, un paradigma de investigación, un método de investigación, una disciplina, un flujo de trabajo o una profesión. 

La ciencia de datos integra el conocimiento del dominio de la aplicación subyacente (por ejemplo, ciencia económica, finanzas, medicina, ciencias naturales, tecnologías de la información) y es un "concepto consistente en unificar estadística, análisis de datos, informática y sus métodos relacionados" para "comprender y analizar fenómenos reales" con datos. 

La ciencia de datos utiliza métodos, técnicas y teorías extraídas de muchos campos dentro del contexto de las matemáticas, las estadísticas, las ciencias de la computación, las ciencias de la información y el conocimiento del dominio. 

La ciencia de datos es diferente de la informática, la estadística y la ciencia de la información, el ganador del premio Turing, Jim Gray, imaginó la ciencia de datos como un "cuarto paradigma" de la ciencia (empírico, teórico, computacional y ahora basado en datos) y afirmó que "todo sobre la ciencia está cambiando debido al impacto de la tecnología de la información" y la avalancha de datos. 

Un científico de datos es el profesional que mediante la escritura y aplicación de código de programación y conocimientos en estadística trabaja en la recolección de datos, la limpieza de datos, la exploración de datos, la modelación de datos, visualización de datos, la implementación de soluciones de aprendizaje automático y en la interpretación de resultados. 

El perfil de un científico de datos proviene de diferentes profesiones o backgrounds: matemáticos, ingenieros, economistas, actuarios, físicos, químicos, y en algunas ocasiones de campos que pudieran parecer muy distantes como la medicina o la sociología. 

En 1962, John W Tukey mencionó el término “Ciencia de Datos” en su artículo “The Future of Data Analysis” al explicar una evolución de la estadística matemática, en este, definió por primera vez el análisis de datos como: “Procedimientos para analizar datos, técnicas para interpretar los resultados de dichos procedimientos, formas de planificar la recopilación de datos para hacer su análisis más fácil, más preciso o acertado, y toda la maquinaria y los resultados de las estadísticas matemáticas que se aplican al análisis de datos". 

En 1977, John W Tukey publicó “Exploratory Data Analysis”, argumentando que era necesario poner más énfasis en el uso de datos para sugerir hipótesis que probar en modelos estadísticos. 

La ciencia de datos ha resultado para muchos una disciplina de reciente creación, pero en la realidad este concepto lo utilizó por primera vez el científico danés Peter Naur en la década de los sesenta como sustituto de las ciencias computacionales.

En 1974 Peter Naur publicó el libro Concise Survey of Computer Methods, donde utiliza ampliamente el concepto ciencia de datos, lo que permitió una utilización más libre en el mundo académico. 

En 1977, el International Association for Statistical Computing (IASC) es establecido como una sección del International Statistical Institute (ISI), “Es la misión de la IASC relacionar la metodología estadística tradicional, tecnología computacional moderna, y el conocimiento de expertos del tema, para convertir datos en información y conocimiento". 

En 1996 el término ‘Ciencia de Datos’ fue utilizado por primera vez en una conferencia llamada "Ciencia de datos, clasificación y métodos relacionados", que tuvo lugar en una reunión de miembros de la ‘International Federation of Classification Societies’ (IFCS) con sede en Kobe, Japón. 

En 1997, Jeff Wu describió al trabajo estadístico como una trilogía conformada por recolección de datos, análisis y modelado de datos, y la toma de decisiones, haciendo la petición de que la estadística fuese renombrada como ciencia de datos, y los estadísticos como científicos de datos. 

En 2001, William S Cleveland introdujo a la ciencia de datos como una disciplina independiente, extendiendo el campo de la estadística para incluir los avances en computación con datos en su artículo "Data science: an action plan for expanding the technical areas of the field of statistics", Cleveland estableció seis áreas técnicas que en su opinión conformarían al campo de la ciencia de datos: investigaciones multidisciplinarias, modelos y métodos para datos, computación con datos, pedagogía, evaluación de herramientas, y teoría. 

En abril del 2002, el ‘International Council for Science: Committee on Data for Science and Technology’ (CODATA) empezó la publicación del Data Science Journal, enfocada en problemas como la descripción de sistemas de datos, su publicación en Internet, sus aplicaciones, y sus problemas legales, poco después, en enero del 2003, la Universidad de Columbia empezó a publicar The Journal of Data Science, la cual ofreció una plataforma para que todos los profesionales de datos presentaran sus perspectivas e intercambiaran ideas. 

En 2005, The National Science Board publicó "Long-Lived Digital Data Collections Enabling Research and Education in the 21st Century", definiendo a los científicos de datos como "científicos de computación e información, programadores de bases de datos y software, y expertos disciplinarios, que son cruciales para la gestión exitosa de una colección digital de datos, cuya actividad primaria es realizar investigación creativa y análisis", fue en el 2008 que Jeff Hammerbacher y DJ Patil lo reutilizaron para definir sus propios trabajos realizados en Facebook y LinkedIn, respectivamente. 

En 2009, los investigadores Yangyong Zhu y Yun Xiong del ‘Research Center for Dataology and Data Science’, publicaron “Introduction to Dataology and Data Science”, en donde manifiestan que “a diferencia de las ciencias naturales y las ciencias sociales, Datología y Ciencia de Datos toman datos en la red y su objeto de estudio” 

En 2013 fue lanzado el ‘IEEE Task Force on Data Science and Advanced Analytics’, mientras que la primera conferencia internacional de ‘IEEE International Conference on Data Science and Advanced Analytics’ fue lanzada en el 2014. 

En 2015, el International Journal on Data Science and Analytics fue lanzado por Springer para publicar trabajos originales en ciencia de datos y analítica de big data. 

Sobre el roadmap para convertirse en científico de datos, existen diferentes formas de adquirir el conocimiento necesario, las universidades están empezando a ofrecer cursos y diplomados y algunas, maestrías y doctorados en ciencia de datos. 

IBM Network Skills ofrece un Certificado Profesional en Ciencia de Datos que tiene un costo (pero se puede pedir ayuda económica para cursarlo gratuitamente), el cual está compuesto por 9 cursos y tiene una duración de 10 meses, más información en https://www.coursera.org/professional-certificates/certificado-profesional-de-ciencia-de-datos-de-ibm. 

La Universidad Peruana de Ciencias Aplicadas a través de su Escuela de Postgrado también ofrece Cursos de Ciencia de Datos, son cursos cortos en la modalidad online y virtual distribuidos en sus categorías Flex Courses, de 6 horas académicas, y Cursos Especializados, de 24 horas. 

Sobre ejemplos de aplicaciones de Ciencia de Datos en el Marketing, en septiembre de 1994, BusinessWeek publicó el artículo “Marketing de base de datos”, manifestando que las empresas recopilan una gran cantidad de información sobre los clientes, la cual es analizada para predecir la probabilidad de que compre un producto, afirman que se utiliza ese conocimiento para elaborar un mensaje de marketing calibrado con precisión para que el individuo busque conseguirlo, asimismo, explican que, en los ochenta, un entusiasmo provocado por la propagación de los lectores de códigos de barras terminó en una decepción generalizada pues muchas empresas fueron abrumadas por la gran cantidad de datos para lograr hacer algo útil con la información de sus clientes, sin embargo, muchas empresas creen que no hay más remedio que desafiar la frontera marketing y bases de datos para desarrollar más las tecnologías necesarias. 

Sobre ejemplos de aplicaciones de Ciencia de Datos en el Marketing, en 2014 la empresa sueca de música en streaming Spotify compra The Echo Nest, una compañía especializada en ciencia de datos musicales, ésta ahora es la encargada de almacenar y analizar la información de sus 170 millones de usuarios, con ayuda de dicha empresa, en 2015 Spotify lanzó un servicio de música personalizada llamado Discover Weekly que semanalmente recomienda a sus usuarios una selección de canciones que podría interesarles por medio de algoritmos y análisis de los datos de la música escuchada y el historial de búsqueda de la semana pasada, el servicio recibió una buena recepción generalizada y actualmente figura un fuerte punto de venta ante la competencia de la empresa. 

Sobre ejemplos de aplicaciones de Ciencia de Datos en el Marketing, Netflix, la empresa norteamericana de contenido multimedia en streaming ofrece a sus más de 120 millones de usuarios una plataforma capaz de analizar, mediante algoritmos, las costumbres de consumo de los usuarios para diferenciar los contenidos que estos buscan y lograr determinar qué nuevos contenidos les pueden interesar, Todd Yellin, vicepresidente de producto en Netflix, explicó que algunos de los datos almacenados pueden extenderse desde la hora del día se conectan sus usuarios, cuánto tiempo pasan dentro de la plataforma, su lista de contenidos recientemente vistos (para analizar incluso el orden específico de estos), toda la información que se almacena es utilizada específicamente para ser analizada, aprender del usuario y poder darle recomendaciones acertadas. 

Sobre ejemplos de aplicaciones de Ciencia de Datos en Gobernanza, en América Latina el Banco Interamericano de Desarrollo (BID) ha desarrollado estudios exploratorios en los que se analiza la ciencia de datos en la implementación y diseño de políticas públicas en la región, tomando casos en países como Argentina y Brasil, presentando recomendaciones para su implementación y mantenimiento, éstas van desde temas como movilidad urbana sostenible, ciudades inteligentes, seguridad, propiedad de datos y privacidad, entre las sugerencias presentadas en las investigaciones está la de lograr una “inteligencia del valor público, la cual “tiene la potencialidad de ser un componente estratégico para la toma de decisiones y el diseño, implementación y evaluación de políticas públicas”, otra de ellas es la capacidad para lograr desde este campo una mejora de rendición de cuentas de los gobiernos ante la ciudadanía y promover un avance en cuanto a la curaduría de datos en las instituciones públicas. 

Textualmente, Big Data se refiere a enormes volúmenes de datos que no pueden procesarse de manera efectiva con las aplicaciones tradicionales que actualmente se aplican, de acuerdo con la guía de Amazon Web Service, se considera al Big Data como una colección considerable de datos con dificultades para almacenarse en bases de datos tradicionales, y también para procesarse en servidores estándar y para analizarse con aplicaciones habituales. 

Big Data se suele relacionar con ciencia de datos, pues esa suele ser su fuente de información para análisis. 

La ciencia de datos logra trabajar y analizar los grandes conjuntos de datos desordenados e incompletos, para llegar a hallazgos que impulsan decisiones sobre operaciones y productos. 

Se define al científico de datos como una mezcla de estadísticos, computólogos y pensadores creativos, con las siguientes habilidades: 

- Recopilar, procesar y extraer valor de las diversas y extensas bases de datos
- Imaginación para comprender, visualizar y comunicar sus conclusiones a los no científicos de datos
- Capacidad para crear soluciones basadas en datos que aumentan los beneficios, reducen los costos. 

El proceso que sigue un científico de datos para trabajar o resolver problemas con los datos se puede resumir en estos pasos: 

- Extraer datos, independientemente de la fuente y de su volumen
- Limpiar los datos, para eliminar lo que pueda sesgar los resultados
- Procesar los datos usando métodos estadísticos como inferencia estadística, modelos de regresión, pruebas de hipótesis, etc
- Diseñar experimentos adicionales en caso de ser necesario
- Crear visualizaciones gráficas de los datos relevantes de la investigación. 

El científico de datos es un estadístico que debería saber o aprender interfaces de programación de aplicaciones (API), bases de datos y extracción de datos; es un diseñador que deberá aprender a programar; y es un computólogo que deberá saber analizar y encontrar datos con significado. 

En la tesis doctoral de Benjamin Fry explicó que el proceso para comprender mejor a los datos comenzaba con una serie de números y el objetivo de responder preguntas sobre los datos, en cada fase del proceso que él propone (adquirir, analizar, filtrar, extraer, representar, refinar e interactuar), se requiere de diferentes enfoques especializados que aporten a una mejor comprensión de los datos, entre los enfoques que menciona Fry están: ingenieros en sistemas, matemáticos, estadísticos, diseñadores gráficos, especialistas en visualización de la información y especialistas en interacciones hombre-máquina, mejor conocidos por sus siglas en inglés “HCI” (Human-Computer Interaction), además, Fry afirmó que contar con diferentes enfoques especializados lejos de resolver el problema de entendimiento de datos, se convierte en parte del problema, ya que cada especialización conduce de manera aislada el problema y el camino hacia la solución se puede perder algo en cada transición del proceso. 

Drew Conway en su página web explica con la ayuda de un diagrama de Venn, las principales habilidades que le dan vida y forma a la ciencia de datos, así como sus relaciones de conjuntos.

La ciencia de datos ha cobrado recientemente mucha importancia en nuestro acontecer como disciplina o profesión emergente (científico de datos), y se ha vuelto en foco de atención de cada vez más organizaciones a nivel mundial, tal como lo señaló el economista en jefe de Google Hal Varian, “El trabajo más sexy en los próximos 10 años será ser estadístico”, palabras sobre las que reflexionó Thomas H Davenport para publicar en el 2012 su artículo: Data Scientist: The Sexiest Job of the 21st Century donde describe el perfil que debe tener el científico de datos como el híbrido de un hacker de datos, un analista, un comunicador, y un consejero confiable, combinación extremadamente poderosa y poco común, Davenport, también señala que el científico de datos no se siente cómodo como se dice coloquialmente “con la correa corta”, es decir, debe tener la libertad de experimentar y explorar posibilidades. 

El informe que publicó “McKinsey” en el 2011, estimó que para el mundo de grandes datos en el que vivimos, espera que la demanda por talento experto en ciencia de datos podría alcanzar de los 440 000 a 490 000 puestos de trabajo para el 2018. 

Entre los retos tecnológicos en Ciencia de Datos a los que nos enfrentamos destacamos:

- El volumen de datos: la genómica, la monitorización (UCI, dispositivos móviles), la ubicuidad, datos sociales, se requerirán, por una parte, nuevos métodos para el almacenamiento de datos; por otra parte, estos datos requieren nuevas aplicaciones para su integración, consulta y análisis
- Almacenamiento físico de los datos: los datos requieren de nuevos medios y arquitecturas para su almacenamiento y tratamiento de forma eficiente
- Problemas de interoperabilidad: diversos hospitales tienen diferentes sistemas de almacenamiento, tiene que haber una capa de interoperabilidad para construir sobre las soluciones de tecnologías de la información
- Limpieza de datos, integración, análisis, herramientas: cuando se tenga acceso a información de todo tipo: los registros de salud, información de contexto, la genómica, y el resto de datos, serán necesarias nuevas herramientas y servicios para diferenciar el ruido de los datos valiosos
- Interpretabilidad de los modelos obtenidos con técnicas de inteligencia artificial, impacto de los cambios en los protocolos de registro de datos y en la normativa sobre los datos registrados. 