{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["9A2oSLDivrbN","kDRIGcy62JuE","qQEIdACV2IE4","_RnqKO9ChOpY","IVJ6UmQV3n4T"],"authorship_tag":"ABX9TyNglfcujGMMCg0g4lxPcoXX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Configurar ambiente"],"metadata":{"id":"9A2oSLDivrbN"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"UD0OObFGtZVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686087541287,"user_tz":180,"elapsed":41498,"user":{"displayName":"Alejandro Gamarra (ElProfeAlejo)","userId":"00472658790543316179"}},"outputId":"82c44476-e775-4e56-920e-ef8885e32563"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-06 21:38:30.736643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-md==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.5.0/es_core_news_md-3.5.0-py3-none-any.whl (42.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.5.0) (3.5.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (8.1.9)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.10.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-md==3.5.0) (2.1.2)\n","Installing collected packages: es-core-news-md\n","Successfully installed es-core-news-md-3.5.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_md')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jellyfish\n","  Downloading jellyfish-0.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jellyfish\n","Successfully installed jellyfish-0.11.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.2)\n","Building wheels for collected packages: python-docx\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184491 sha256=c702dec5881696a3073f6103577baed634f7bd073796628f529c33a8a53dbad3\n","  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n","Successfully built python-docx\n","Installing collected packages: python-docx\n","Successfully installed python-docx-0.8.11\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"]}],"source":["#Instalando bibliotecas necesarias\n","import pandas as pd\n","import numpy as np\n","import re\n","import os\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import spacy\n","!python -m spacy download es_core_news_md\n","nlp = spacy.load('es_core_news_md')\n","!pip install jellyfish\n","import jellyfish\n","import requests\n","!pip install python-docx\n","import csv\n","from docx import Document\n","import nltk\n","nltk.download('punkt')\n","global diccionario_irregulares, documento, lista_frases, lista_frases_normalizadas\n","import warnings, os\n","warnings.filterwarnings('ignore')\n","!pip install transformers\n","from transformers import BertForSequenceClassification\n","from transformers import BertTokenizer\n","from bs4 import BeautifulSoup\n","import torch\n","import random"]},{"cell_type":"markdown","source":["#2. Tratamiento de datos"],"metadata":{"id":"kDRIGcy62JuE"}},{"cell_type":"code","source":["#Función para encontrar la raiz de las palabras\n","def raiz(palabra):\n","  radio=0\n","  palabra_encontrada=palabra\n","  for word in lista_verbos:\n","    confianza = jellyfish.jaro_winkler(palabra, word)\n","    if (confianza>=0.93 and confianza>=radio):\n","      radio=confianza\n","      palabra_encontrada=word\n","  return palabra_encontrada\n","\n","def tratamiento_texto(texto):\n","  trans = str.maketrans('áéíóú','aeiou')\n","  texto = texto.lower()\n","  texto = texto.translate(trans)\n","  texto = \" \".join(texto.split())\n","  return texto\n","\n","#Función para reemplazar el final de una palabra por 'r'\n","def reemplazar_terminacion(palabra):\n","  patron = r\"(es|me|as|te|ste)$\"\n","  nueva_palabra = re.sub(patron, \"r\", palabra)\n","  return nueva_palabra.split()[0]\n","\n","#Función para adicionar o eliminar tokens\n","def revisar_tokens(texto, tokens):\n","  if len(tokens)==0:\n","    if [x for x in ['elprofealejo', 'el profe alejo', 'profe alejo', 'profealejo'] if x in tratamiento_texto(texto)]: tokens.append('elprofealejo')\n","    elif [x for x in ['cientifico de datos', 'data scientist'] if x in tratamiento_texto(texto)]: tokens.append('datascientist')\n","    elif [x for x in ['ciencia de datos', 'data science'] if x in tratamiento_texto(texto)]: tokens.append('datascience')\n","    elif [x for x in ['big data', 'bigdata'] if x in tratamiento_texto(texto)]: tokens.append('bigdata')\n","  else:\n","    elementos_a_eliminar = [\"profe\", \"alejo\", \"profealejo\", \"cual\", \"que\", \"quien\", \"cuanto\", \"donde\", \"cuando\", \"como\"]\n","    if 'hablame' in texto and 'hablar' in tokens: tokens.remove('hablar')\n","    elif 'cuentame' in texto and 'contar' in tokens: tokens.remove('contar') \n","    elif 'hago' in texto and 'hacer' in tokens: tokens.remove('hacer') \n","    elif 'entiendes' in texto and 'entender' in tokens: tokens.remove('entender') \n","    elif 'sabes' in texto and 'saber' in tokens: tokens.remove('saber') \n","    tokens = [x.replace('datar','data').replace('datos','dato') for x in tokens if x not in elementos_a_eliminar]\n","  return tokens\n","\n","#Función para devolver los tokens normalizados del texto\n","def normalizar(texto):\n","  tokens=[]\n","  tokens=revisar_tokens(texto, tokens)\n","  if 'elprofealejo' in tokens:\n","    texto = ' '.join(texto.split()[:15])\n","  else:\n","    texto = ' '.join(texto.split()[:25])\n","\n","  doc = nlp(texto)\n","  for t in doc:\n","    lemma=diccionario_irregulares.get(t.text, t.lemma_.split()[0])\n","    lemma=re.sub(r'[^\\w\\s+\\-*/]', '', lemma)\n","    if t.pos_ in ('VERB','PROPN','PRON','NOUN','AUX','SCONJ','ADJ','ADV','NUM') or lemma in lista_verbos:\n","      if t.pos_=='VERB':\n","        lemma = reemplazar_terminacion(lemma)\n","        tokens.append(raiz(tratamiento_texto(lemma)))\n","      else:\n","        tokens.append(tratamiento_texto(lemma))\n","\n","  tokens = list(dict.fromkeys(tokens))\n","  tokens = list(filter(None, tokens))\n","  tokens = revisar_tokens(texto, tokens)\n","  return tokens\n","\n","#Función normalizar que se utilizó para entrenar el modelo\n","def normalizar_modelo(texto):\n","  doc = nlp(texto)\n","  tokens=[]\n","  if len(doc)<=3:\n","    for t in doc:\n","      if t.pos_=='VERB':\n","        tokens.append(raiz(t.lemma_))\n","      else:\n","        tokens.append(t.lemma_)\n","  else:\n","    for t in doc:\n","      if (t.pos_ in ('VERB','PROPN','PRON','NOUN','AUX','SCONJ','DET','ADJ','ADV') or any(t.dep_.startswith(elemento) for elemento in ['ROOT'])):\n","        if t.pos_=='VERB':\n","          tokens.append(raiz(t.lemma_))\n","        else:\n","          tokens.append(t.lemma_)\n","  tokens = list(dict.fromkeys(tokens))\n","  tokens = tokens[:10]\n","  tokens = ' '.join(tokens)\n","  return tratamiento_texto(tokens)"],"metadata":{"id":"N-__6F0I2MjG","executionInfo":{"status":"ok","timestamp":1686090774273,"user_tz":180,"elapsed":402,"user":{"displayName":"Alejandro Gamarra (ElProfeAlejo)","userId":"00472658790543316179"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["# 3. Cargar bases de verbos"],"metadata":{"id":"qQEIdACV2IE4"}},{"cell_type":"code","source":["#Importando verbos en español\n","headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n","trans = str.maketrans('áéíóú','aeiou')\n","lista_verbos=[]\n","url = ['https://www.ejemplos.co/verbos-mas-usados-en-espanol/',\n","       'https://www.ejemplos.co/verbos-predicativos/',\n","       'https://www.ejemplos.co/verbos-personales/',\n","       'https://www.ejemplos.co/verbos-irregulares/',\n","       'https://www.ejemplos.co/verbos/',\n","       'https://www.ejemplos.co/100-ejemplos-de-verbos-regulares/',\n","       'https://www.ejemplos.co/verbos-del-decir/',\n","       'https://www.ejemplos.co/verbos-con-a/',\n","       'https://www.ejemplos.co/verbos-con-b/',\n","       'https://www.ejemplos.co/verbos-con-c/',\n","       'https://www.ejemplos.co/verbos-con-d/',\n","       'https://www.ejemplos.co/verbos-con-e/',\n","       'https://www.ejemplos.co/verbos-con-f/',\n","       'https://www.ejemplos.co/verbos-con-g/',\n","       'https://www.ejemplos.co/verbos-con-h/',\n","       'https://www.ejemplos.co/verbos-con-i/',\n","       'https://www.ejemplos.co/verbos-con-j/',\n","       'https://www.ejemplos.co/verbos-con-k/',\n","       'https://www.ejemplos.co/verbos-con-l/',\n","       'https://www.ejemplos.co/verbos-con-ll/',\n","       'https://www.ejemplos.co/verbos-con-m/',\n","       'https://www.ejemplos.co/verbos-con-n/',\n","       'https://www.ejemplos.co/verbos-con-o/',\n","       'https://www.ejemplos.co/verbos-con-p/',\n","       'https://www.ejemplos.co/verbos-con-q/',\n","       'https://www.ejemplos.co/verbos-con-r/',\n","       'https://www.ejemplos.co/verbos-con-s/',\n","       'https://www.ejemplos.co/verbos-con-t/',\n","       'https://www.ejemplos.co/verbos-con-u/',\n","       'https://www.ejemplos.co/verbos-con-v/',\n","       'https://www.ejemplos.co/verbos-con-w/',\n","       'https://www.ejemplos.co/verbos-con-x/',\n","       'https://www.ejemplos.co/verbos-con-y/',\n","       'https://www.ejemplos.co/verbos-con-z/']\n","\n","for i in range(len(url)):\n","  try:\n","    respuesta = requests.get(url[i], headers=headers)\n","    respuesta = respuesta.content.decode('utf-8')\n","    bases = pd.read_html(respuesta)\n","    for i, df in enumerate(bases):\n","      for idx,row in bases[i].iterrows():     \n","          _ = [lista_verbos.append(re.sub(r\"\\((.*?)\\)\", '', x.lower()).strip().translate(trans)) for x in row[0].split('/')]\n","          _ = [lista_verbos.append(re.sub(r\"\\((.*?)\\)\", '', x.lower()).strip().translate(trans)) for x in row[1].split('/')]\n","          _ = [lista_verbos.append(re.sub(r\"\\((.*?)\\)\", '', x.lower()).strip().translate(trans)) for x in row[2].split('/')]\n","  except Exception:\n","    continue\n","lista_verbos = [elemento for elemento in lista_verbos if (elemento=='ir' or len(elemento)!=2)]\n","nuevos_verbos = ['costar', 'referir', 'datar']\n","lista_verbos.extend(nuevos_verbos)\n","lista_verbos=list(set(lista_verbos))\n","\n","# Definir una lista de verbos irregulares y sus conjugaciones en pasado, presente, futuro, imperfecto, pretérito y condicional\n","verbos_irregulares = [\n","    ('ser', 'soy', 'eres', 'seras', 'eras', 'es', 'serias'),\n","    ('estar', 'estuviste', 'estas', 'estaras', 'estabas', 'estuviste', 'estarias'),\n","    ('ir', 'fuiste', 'vas', 'iras', 'ibas', 'fuiste', 'irias'),\n","    ('ir', 'fuiste', 'vaya', 'iras', 'ibas', 'fuiste', 'irias'),\n","    ('tener', 'tuviste', 'tienes', 'tendras', 'tenias', 'tuviste', 'tendrias'),\n","    ('hacer', 'hiciste', 'haces', 'haras', 'hacias', 'hiciste', 'harias'),\n","    ('decir', 'dijiste', 'dices', 'diras', 'decias', 'dijiste', 'dirias'),\n","    ('decir', 'dimar', 'dime', 'digame', 'dimir', 'dimo', 'dimiria'),\n","    ('poder', 'pudiste', 'puedes', 'podras', 'podias', 'pudiste', 'podrias'),\n","    ('saber', 'supiste', 'sabes', 'sabras', 'sabias', 'supiste', 'sabrias'),\n","    ('poner', 'pusiste', 'pones', 'pondras', 'ponias', 'pusiste', 'pondrias'),\n","    ('ver', 'viste', 'ves', 'veras', 'veias', 'viste', 'verias'),\n","    ('dar', 'diste', 'das', 'daras', 'dabas', 'diste', 'darias'),\n","    ('dar', 'damar', 'dame', 'daras', 'dabas', 'darme', 'darias'),\n","    ('venir', 'viniste', 'vienes', 'vendras', 'venias', 'viniste', 'vendrias'),\n","    ('haber', 'haya', 'has', 'habras', 'habias', 'hubiste', 'habrias'),\n","    ('caber', 'cupiste', 'cabes', 'cabras', 'cabias', 'cupiste', 'cabrias'),\n","    ('valer', 'valiste', 'vales', 'valdras', 'valias', 'valiste', 'valdrias'),\n","    ('querer', 'quisiste', 'quieres', 'querras', 'querias', 'quisiste', 'querrias'),\n","    ('llegar', 'llegaste', 'llegares', 'llegaras', 'llegarias', 'llegaste', 'llegarrias'),\n","    ('hacer', 'hiciste', 'haces', 'haras', 'hacias', 'hiciste', 'harias'),\n","    ('decir', 'dijiste', 'dices', 'diras', 'decias', 'dijiste', 'dirias'),\n","    ('poder', 'pudiste', 'puedes', 'podras', 'podias', 'pudiste', 'podria'),\n","    ('contar', 'contaste', 'cuentas', 'contaras', 'contabas', 'cuentame', 'contarias'),\n","    ('saber', 'supiste', 'sabes', 'sabras', 'sabias', 'supiste', 'sabrias'),\n","    ('costar', 'cuesta', 'cuestan', 'costo', 'costaria', 'costarian', 'cuestas'),\n","    ('durar', 'duraste', 'duro', 'duraras', 'durabas', 'duraste', 'durarias')\n","]\n","\n","# Crear el DataFrame\n","diccionario_irregulares = {}\n","df = pd.DataFrame(verbos_irregulares, columns=['Verbo', 'Pasado', 'Presente', 'Futuro', 'Imperfecto', 'Pretérito', 'Condicional'])\n","for columna in df.columns:\n","  if columna != 'Verbo':\n","    for valor in df[columna]:\n","      diccionario_irregulares[valor] = df.loc[df[columna] == valor, 'Verbo'].values[0]"],"metadata":{"id":"8DDVG_F22IvU","executionInfo":{"status":"ok","timestamp":1686087617769,"user_tz":180,"elapsed":3759,"user":{"displayName":"Alejandro Gamarra (ElProfeAlejo)","userId":"00472658790543316179"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# 4. Cargar bases de documentos"],"metadata":{"id":"_RnqKO9ChOpY"}},{"cell_type":"code","source":["#Importando bases de dialogo fluído\n","txt_folder_path = '/content/dialogos'\n","lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n","lista_dialogos, lista_dialogos_respuesta, lista_tipo_dialogo = [],[],[]\n","for idx in range(len(lista_documentos)):\n","  f=open(txt_folder_path+'/'+lista_documentos[idx], 'r', encoding='utf-8', errors='ignore')\n","  flag,posicion = True,0\n","  for line in f.read().split('\\n'):\n","    if flag:\n","      line = tratamiento_texto(line)\n","      line = re.sub(r\"[^\\w\\s]\", '', line)\n","      lista_dialogos.append(line)\n","      lista_tipo_dialogo.append(lista_documentos[idx].replace('.txt', ''))\n","    else:\n","      lista_dialogos_respuesta.append(line)\n","      posicion+=1\n","    flag=not flag\n","\n","#Creando Dataframe de diálogos\n","datos = {'dialogo':lista_dialogos,'respuesta':lista_dialogos_respuesta,'tipo':lista_tipo_dialogo,'interseccion':0,'similarity':0,'jaro_winkler':0,'probabilidad':0}\n","df_dialogo = pd.DataFrame(datos)\n","df_dialogo = df_dialogo.drop_duplicates(keep='first')\n","df_dialogo.reset_index(drop=True, inplace=True)\n","\n","#Importando bases csv\n","txt_folder_path = '/content/documentos'\n","lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".csv\")]\n","documento_csv = ''\n","for idx in range(len(lista_documentos)):\n","  with open(txt_folder_path+'/'+lista_documentos[idx], \"r\", encoding=\"utf-8\") as archivo_csv:\n","    lector_csv = csv.reader(archivo_csv)\n","    for fila in lector_csv:\n","      if fila[-1]!='frase':\n","        documento_csv += fila[-1]\n","\n","#Importando bases docx\n","txt_folder_path = '/content/documentos'\n","lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".docx\")]\n","documento_docx = ''\n","for idx in range(len(lista_documentos)):\n","  for paragraph in Document(txt_folder_path+'/'+lista_documentos[idx]).paragraphs:\n","    documento_docx += paragraph.text.replace('*','\\n\\n*').replace('-','\\n-')\n","\n","#Importando bases txt\n","txt_folder_path = '/content/documentos'\n","lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n","documento_txt = ''\n","for idx in range(len(lista_documentos)):\n","  with open(txt_folder_path+'/'+lista_documentos[idx], \"r\", encoding=\"utf-8\") as archivo_txt:\n","    lector_txt = archivo_txt.read()\n","    for fila in lector_txt:\n","      documento_txt += fila\n","\n","documento = documento_csv + documento_txt + documento_docx\n","lista_frases = nltk.sent_tokenize(documento,'spanish')\n","lista_frases_normalizadas = [' '.join(normalizar(x)) for x in lista_frases]"],"metadata":{"id":"1894SyvPhQJ0","executionInfo":{"status":"ok","timestamp":1686088549202,"user_tz":180,"elapsed":1132,"user":{"displayName":"Alejandro Gamarra (ElProfeAlejo)","userId":"00472658790543316179"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# 5. Buscar respuesta del Chatbot"],"metadata":{"id":"IVJ6UmQV3n4T"}},{"cell_type":"code","source":["#Función para verificar si el usuário inició un diálogo\n","def dialogo(user_response):\n","  user_response = tratamiento_texto(user_response)\n","  user_response = re.sub(r\"[^\\w\\s]\", '', user_response)\n","  df = df_dialogo.copy()\n","  vectorizer = TfidfVectorizer()\n","  dialogos_numero = vectorizer.fit_transform(df_dialogo['dialogo'])\n","  respuesta_numero = vectorizer.transform([user_response])\n","  for idx,row in df.iterrows():\n","    df.at[idx,'interseccion'] = len(set(user_response.split()) & set(row['dialogo'].split()))/len(user_response.split())\n","    df.at[idx,'similarity'] = cosine_similarity(dialogos_numero[idx], respuesta_numero)[0][0]\n","    df.at[idx,'jaro_winkler'] = jellyfish.jaro_winkler(user_response,row['dialogo'])\n","    df.at[idx,'probabilidad'] = max(df.at[idx,'interseccion'],df.at[idx,'similarity'],df.at[idx,'jaro_winkler'])\n","  df.sort_values(by=['probabilidad','jaro_winkler'], inplace=True, ascending=False)\n","  probabilidad = df['probabilidad'].head(1).values[0]\n","  tipo = df['tipo'].head(1).values[0]\n","  if probabilidad >= 0.93 and tipo not in ['ElProfeAlejo']:\n","    print('Respuesta encontrada por el método de comparación de textos - Probabilidad: ', probabilidad)\n","    respuesta = df['respuesta'].head(1).values[0]\n","  else:\n","    respuesta = ''\n","  return respuesta\n","\n","#Cargar el modelo entrenado\n","ruta_modelo = '/content/modelo'\n","Modelo_TF = BertForSequenceClassification.from_pretrained(ruta_modelo)\n","tokenizer_TF = BertTokenizer.from_pretrained(ruta_modelo)\n","\n","#Función para dialogar utilizando el modelo Transformers\n","def clasificacion_modelo(pregunta):\n","  pregunta = re.sub(r\"[^\\w\\s]\", '', pregunta)\n","  frase = normalizar_modelo(pregunta)\n","  tokens = tokenizer_TF.encode_plus(\n","      frase,\n","      add_special_tokens=True,\n","      max_length=128,\n","      padding='max_length',\n","      truncation=True,\n","      return_tensors='pt'\n","  )\n","  input_ids = tokens['input_ids']\n","  attention_mask = tokens['attention_mask']\n","\n","  with torch.no_grad():\n","      outputs = Modelo_TF(input_ids, attention_mask)\n","\n","  etiquetas_predichas = torch.argmax(outputs.logits, dim=1)\n","  etiquetas_decodificadas = etiquetas_predichas.tolist()\n","\n","  diccionario = {3: 'Continuacion', 10: 'Nombre', 2: 'Contacto', 13: 'Saludos', 14: 'Sentimiento', 9: 'Identidad', 15: 'Usuario', 6: 'ElProfeAlejo', 1: 'Aprendizaje', 0: 'Agradecimiento', 5: 'Edad', 4: 'Despedida', 11: 'Origen', 12: 'Otros', 7: 'Error', 8: 'Funcion'}\n","  llave_buscada = etiquetas_decodificadas[0]\n","  clase_encontrada = diccionario[llave_buscada]\n","\n","  #Buscar respuesta más parecida en la clase encontrada\n","  df = df_dialogo[df_dialogo['tipo'] == clase_encontrada]\n","  df.reset_index(inplace=True)\n","  vectorizer = TfidfVectorizer()\n","  dialogos_num = vectorizer.fit_transform(df['dialogo'])\n","  pregunta_num = vectorizer.transform([tratamiento_texto(pregunta)])\n","  similarity_scores = cosine_similarity(dialogos_num, pregunta_num)\n","  indice_pregunta_proxima = similarity_scores.argmax()\n","  if clase_encontrada not in ['Otros','ElProfeAlejo']:\n","    print('Respuesta encontrada por el modelo Transformers - tipo:',clase_encontrada)\n","    respuesta = df['respuesta'][indice_pregunta_proxima]\n","  else:\n","    respuesta = ''\n","  return respuesta\n","\n","#Función para devolver la respuesta de los documentos\n","def respuesta_documento(pregunta):\n","  pregunta = normalizar(pregunta)\n","  def contar_coincidencias(frase):\n","    return sum(1 for elemento in pregunta if elemento in frase) \n","\n","  diccionario = {valor: posicion for posicion, valor in enumerate(lista_frases_normalizadas)}\n","  lista = sorted(list(diccionario.keys()), key=contar_coincidencias, reverse=True)[:6]\n","  if 'curso' not in pregunta: lista = [frase for frase in lista if 'curso' not in frase]\n","  lista.append(' '.join(pregunta))\n","  TfidfVec = TfidfVectorizer(tokenizer=normalizar)\n","  tfidf = TfidfVec.fit_transform(lista)\n","  vals = cosine_similarity(tfidf[-1], tfidf)\n","  idx = vals.argsort()[0][-2]\n","  flat = vals.flatten()\n","  flat.sort()\n","  req_tfidf = round(flat[-2],2)\n","  if req_tfidf>=0.22:\n","    print('Respuesta encontrada por el método TfidfVectorizer - Probabilidad:', req_tfidf)\n","    respuesta = lista_frases[diccionario[lista[idx]]]\n","  else:\n","    respuesta = ''\n","  return respuesta\n","\n","#Función para devolver una respuesta final buscada en todos los métodos disponibles\n","def respuesta_chatbot(pregunta):\n","  respuesta = dialogo(pregunta)\n","  if respuesta != '':\n","    return respuesta\n","  else:\n","    respuesta = respuesta_documento(pregunta)\n","    if respuesta != '':\n","      return respuesta\n","    else:\n","      respuesta = clasificacion_modelo(pregunta)\n","      if respuesta != '':\n","        return respuesta\n","      else:\n","        return 'Respuesta no encontrada'"],"metadata":{"id":"Bpxajyck3pav","executionInfo":{"status":"ok","timestamp":1686093704327,"user_tz":180,"elapsed":1735,"user":{"displayName":"Alejandro Gamarra (ElProfeAlejo)","userId":"00472658790543316179"}}},"execution_count":143,"outputs":[]},{"cell_type":"markdown","source":["# 6. Ejecutar Chatbot"],"metadata":{"id":"LMEwexpz4gdI"}},{"cell_type":"code","source":["pregunta='que es Machine Learning?'\n","respuesta = respuesta_chatbot(pregunta)\n","print(respuesta)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_wq8HMP4hmU","executionInfo":{"status":"ok","timestamp":1686100188830,"user_tz":180,"elapsed":3197,"user":{"displayName":"Alejandro Gamarra (ElProfeAlejo)","userId":"00472658790543316179"}},"outputId":"fc8872b0-51d9-4ae2-d611-676affd4a07a"},"execution_count":160,"outputs":[{"output_type":"stream","name":"stdout","text":["Respuesta no encontrada\n"]}]}]}